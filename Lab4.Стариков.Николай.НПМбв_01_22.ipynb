{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T19:56:33.367714Z",
     "start_time": "2026-02-09T19:56:32.578888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import line_search\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "#1.1\n",
    "def gradient_descent(oracle, x0, tolerance=1e-5, max_iter=1000,\n",
    "                     line_search_options=None, trace=False):\n",
    "    \"\"\"\n",
    "    Градиентный спуск с различными стратегиями выбора шага.\n",
    "    \"\"\"\n",
    "    if line_search_options is None:\n",
    "        line_search_options = {'method': 'Wolfe', 'c1': 1e-4, 'c2': 0.9}\n",
    "\n",
    "    x_k = np.copy(x0)\n",
    "    grad_0 = oracle.grad(x0)\n",
    "    grad_norm_0_sq = np.dot(grad_0, grad_0)\n",
    "\n",
    "    alpha = 1.0  # начальная длина шага\n",
    "\n",
    "    if trace:\n",
    "        history = {'time': [], 'func': [], 'grad_norm': [], 'x': []}\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        # Вычисление градиента\n",
    "        f_k = oracle.func(x_k)\n",
    "        grad_k = oracle.grad(x_k)\n",
    "        grad_norm_sq = np.dot(grad_k, grad_k)\n",
    "\n",
    "        if trace:\n",
    "            history['func'].append(f_k)\n",
    "            history['grad_norm'].append(np.sqrt(grad_norm_sq))\n",
    "            history['x'].append(np.copy(x_k))\n",
    "\n",
    "        # Критерий остановки\n",
    "        if grad_norm_sq <= tolerance * grad_norm_0_sq:\n",
    "            message = f\"success after {k} iterations, gradient norm {np.sqrt(grad_norm_sq):.2e}\"\n",
    "            return (x_k, message, history) if trace else (x_k, message)\n",
    "\n",
    "        # Направление спуска (антиградиент)\n",
    "        d_k = -grad_k\n",
    "\n",
    "        # Линейный поиск\n",
    "        method = line_search_options.get('method', 'Wolfe')\n",
    "\n",
    "        if method == 'Wolfe':\n",
    "            # Используем условия Вульфа\n",
    "            def phi(alpha):\n",
    "                return oracle.func(x_k + alpha * d_k)\n",
    "\n",
    "            def phi_grad(alpha):\n",
    "                return oracle.grad(x_k + alpha * d_k).dot(d_k)\n",
    "\n",
    "            alpha, fc, gc, f_new, g_new, _ = line_search(\n",
    "                phi, phi_grad, f_k, grad_k.dot(d_k),\n",
    "                c1=line_search_options.get('c1', 1e-4),\n",
    "                c2=line_search_options.get('c2', 0.9),\n",
    "                maxiter=50\n",
    "            )\n",
    "\n",
    "            if alpha is None:\n",
    "                # Если line_search не нашел шаг, используем бэктрекинг\n",
    "                alpha = 1.0\n",
    "                c = line_search_options.get('c1', 1e-4)\n",
    "                while phi(alpha) > f_k + c * alpha * grad_k.dot(d_k):\n",
    "                    alpha /= 2\n",
    "\n",
    "        elif method == 'Armijo':\n",
    "            # Условие Армихо с бэктрекингом\n",
    "            alpha = line_search_options.get('initial_alpha', 1.0)\n",
    "            c = line_search_options.get('c1', 1e-4)\n",
    "\n",
    "            while oracle.func(x_k + alpha * d_k) > f_k + c * alpha * grad_k.dot(d_k):\n",
    "                alpha /= 2\n",
    "\n",
    "        elif method == 'Constant':\n",
    "            # Постоянный шаг\n",
    "            alpha = line_search_options.get('alpha', 0.01)\n",
    "\n",
    "        # Обновление точки\n",
    "        x_k = x_k + alpha * d_k\n",
    "\n",
    "    message = f\"iterations limit exceeded after {max_iter} iterations\"\n",
    "    return (x_k, message, history) if trace else (x_k, message)\n",
    "\n",
    "#1.2\n",
    "def newton(oracle, x0, tolerance=1e-5, max_iter=100,\n",
    "           line_search_options=None, trace=False):\n",
    "    \"\"\"\n",
    "    Метод Ньютона с различными стратегиями выбора шага.\n",
    "    \"\"\"\n",
    "    if line_search_options is None:\n",
    "        line_search_options = {'method': 'Wolfe', 'c1': 1e-4, 'c2': 0.9}\n",
    "\n",
    "    x_k = np.copy(x0)\n",
    "    grad_0 = oracle.grad(x0)\n",
    "    grad_norm_0_sq = np.dot(grad_0, grad_0)\n",
    "\n",
    "    if trace:\n",
    "        history = {'time': [], 'func': [], 'grad_norm': [], 'x': []}\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        # Вычисление градиента и гессиана\n",
    "        f_k = oracle.func(x_k)\n",
    "        grad_k = oracle.grad(x_k)\n",
    "        hess_k = oracle.hess(x_k)\n",
    "\n",
    "        grad_norm_sq = np.dot(grad_k, grad_k)\n",
    "\n",
    "        if trace:\n",
    "            history['func'].append(f_k)\n",
    "            history['grad_norm'].append(np.sqrt(grad_norm_sq))\n",
    "            history['x'].append(np.copy(x_k))\n",
    "\n",
    "        # Критерий остановки\n",
    "        if grad_norm_sq <= tolerance * grad_norm_0_sq:\n",
    "            message = f\"success after {k} iterations, gradient norm {np.sqrt(grad_norm_sq):.2e}\"\n",
    "            return (x_k, message, history) if trace else (x_k, message)\n",
    "\n",
    "        try:\n",
    "            # Решение системы Холецкого для нахождения направления Ньютона\n",
    "            L, lower = cho_factor(hess_k)\n",
    "            d_k = -cho_solve((L, lower), grad_k)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Если матрица не положительно определена, используем антиградиент\n",
    "            d_k = -grad_k\n",
    "\n",
    "        # Линейный поиск (всегда начинаем с alpha=1 для метода Ньютона)\n",
    "        alpha = 1.0\n",
    "        method = line_search_options.get('method', 'Wolfe')\n",
    "\n",
    "        if method == 'Wolfe':\n",
    "            # Условия Вульфа\n",
    "            c1 = line_search_options.get('c1', 1e-4)\n",
    "            c2 = line_search_options.get('c2', 0.9)\n",
    "\n",
    "            def phi(a):\n",
    "                return oracle.func(x_k + a * d_k)\n",
    "\n",
    "            phi_0 = f_k\n",
    "            phi_grad_0 = grad_k.dot(d_k)\n",
    "\n",
    "            # Проверяем alpha=1 сначала\n",
    "            phi_1 = phi(1.0)\n",
    "            if phi_1 <= phi_0 + c1 * phi_grad_0:\n",
    "                # Проверяем второе условие Вульфа\n",
    "                grad_1 = oracle.grad(x_k + d_k)\n",
    "                phi_grad_1 = grad_1.dot(d_k)\n",
    "                if abs(phi_grad_1) <= c2 * abs(phi_grad_0):\n",
    "                    alpha = 1.0\n",
    "                else:\n",
    "                    # Если второе условие не выполнено, ищем подходящий alpha\n",
    "                    alpha = 0.5\n",
    "                    while phi(alpha) > phi_0 + c1 * alpha * phi_grad_0:\n",
    "                        alpha /= 2\n",
    "            else:\n",
    "                # Условие Армихо не выполнено для alpha=1\n",
    "                alpha = 0.5\n",
    "                while phi(alpha) > phi_0 + c1 * alpha * phi_grad_0:\n",
    "                    alpha /= 2\n",
    "\n",
    "        elif method == 'Armijo':\n",
    "            # Условие Армихо\n",
    "            c = line_search_options.get('c1', 1e-4)\n",
    "            alpha = 1.0\n",
    "            while oracle.func(x_k + alpha * d_k) > f_k + c * alpha * grad_k.dot(d_k):\n",
    "                alpha /= 2\n",
    "\n",
    "        # Обновление точки\n",
    "        x_k = x_k + alpha * d_k\n",
    "\n",
    "    message = f\"iterations limit exceeded after {max_iter} iterations\"\n",
    "    return (x_k, message, history) if trace else (x_k, message)\n",
    "\n",
    "#1.3\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "from scipy.special import expit\n",
    "import scipy.sparse as sp\n",
    "\n",
    "class LogRegL2Oracle:\n",
    "    \"\"\"Оракул для задачи двухклассовой логистической регрессии с L2-регуляризацией.\"\"\"\n",
    "\n",
    "    def __init__(self, A, b, regcoef):\n",
    "        \"\"\"\n",
    "        Инициализация оракула.\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.regcoef = regcoef\n",
    "        self.m = A.shape[0]\n",
    "        self.n = A.shape[1]\n",
    "\n",
    "    def func(self, x):\n",
    "        \"\"\"Значение функции в точке x.\"\"\"\n",
    "        # Вычисляем Ax (эффективно для плотных и разреженных матриц)\n",
    "        if issparse(self.A):\n",
    "            Ax = self.A.dot(x)\n",
    "        else:\n",
    "            Ax = self.A @ x\n",
    "\n",
    "        # Вычисляем b_i * <a_i, x>\n",
    "        bAx = self.b * Ax\n",
    "\n",
    "        # Логистическая функция потерь\n",
    "        # log(1 + exp(-z)) вычисляем через специальную функцию\n",
    "        log_loss = np.logaddexp(0, -bAx).mean()\n",
    "\n",
    "        # L2 регуляризация\n",
    "        regularization = 0.5 * self.regcoef * np.dot(x, x)\n",
    "\n",
    "        return log_loss + regularization\n",
    "\n",
    "    def grad(self, x):\n",
    "        \"\"\"Градиент функции в точке x.\"\"\"\n",
    "        # Вычисляем Ax\n",
    "        if issparse(self.A):\n",
    "            Ax = self.A.dot(x)\n",
    "        else:\n",
    "            Ax = self.A @ x\n",
    "\n",
    "        # Вычисляем sigma(-b_i * <a_i, x>) = 1 / (1 + exp(b_i * <a_i, x>))\n",
    "        # что равно expit(-b_i * <a_i, x>)\n",
    "        bAx = self.b * Ax\n",
    "        sigma = expit(-bAx)\n",
    "\n",
    "        # Градиент логистической функции потерь\n",
    "        if issparse(self.A):\n",
    "            # Для разреженной матрицы используем специальный метод\n",
    "            grad_log_loss = -self.A.T.dot((sigma * self.b) / self.m)\n",
    "        else:\n",
    "            grad_log_loss = -self.A.T @ (sigma * self.b) / self.m\n",
    "\n",
    "        # Градиент регуляризации\n",
    "        grad_reg = self.regcoef * x\n",
    "\n",
    "        return grad_log_loss + grad_reg\n",
    "\n",
    "    def hess(self, x):\n",
    "        \"\"\"Гессиан функции в точке x.\"\"\"\n",
    "        # Вычисляем Ax\n",
    "        if issparse(self.A):\n",
    "            Ax = self.A.dot(x)\n",
    "        else:\n",
    "            Ax = self.A @ x\n",
    "\n",
    "        # Вычисляем sigma(-b_i * <a_i, x>)\n",
    "        bAx = self.b * Ax\n",
    "        sigma = expit(-bAx)\n",
    "\n",
    "        # Вычисляем диагональную матрицу D = diag(sigma * (1 - sigma))\n",
    "        D = sigma * (1 - sigma) / self.m\n",
    "\n",
    "        if issparse(self.A):\n",
    "            # Для разреженной матрицы: A^T * D * A\n",
    "            # Эффективное вычисление через поэлементное умножение\n",
    "            A_scaled = self.A.multiply(np.sqrt(D)[:, np.newaxis])\n",
    "            hess_log_loss = A_scaled.T.dot(A_scaled)\n",
    "\n",
    "            # Добавляем регуляризацию\n",
    "            hess_reg = self.regcoef * sp.eye(self.n, format='csr')\n",
    "            return hess_log_loss + hess_reg\n",
    "        else:\n",
    "            # Для плотной матрицы\n",
    "            hess_log_loss = self.A.T @ (D[:, np.newaxis] * self.A)\n",
    "            hess_reg = self.regcoef * np.eye(self.n)\n",
    "            return hess_log_loss + hess_reg\n",
    "\n",
    "    def func_directional(self, x, d, alpha):\n",
    "        \"\"\"Значение функции вдоль направления d.\"\"\"\n",
    "        return self.func(x + alpha * d)\n",
    "\n",
    "    def grad_directional(self, x, d, alpha):\n",
    "        \"\"\"Производная по направлению d в точке x + alpha*d.\"\"\"\n",
    "        point = x + alpha * d\n",
    "        grad = self.grad(point)\n",
    "        return np.dot(grad, d)\n",
    "\n",
    "\n",
    "def create_log_reg_oracle(A, b, regcoef):\n",
    "    \"\"\"Создает оракул для логистической регрессии.\"\"\"\n",
    "    return LogRegL2Oracle(A, b, regcoef)\n",
    "\n",
    "#1.4\n",
    "def grad_finite_diff(oracle, x, eps=1e-7):\n",
    "    \"\"\"Вычисление градиента методом конечных разностей.\"\"\"\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        e_i = np.zeros(n)\n",
    "        e_i[i] = 1.0\n",
    "\n",
    "        f_plus = oracle.func(x + eps * e_i)\n",
    "        f_minus = oracle.func(x - eps * e_i)\n",
    "\n",
    "        grad[i] = (f_plus - f_minus) / (2 * eps)\n",
    "\n",
    "    return grad\n",
    "\n",
    "def hess_finite_diff(oracle, x, eps=1e-5):\n",
    "    \"\"\"Вычисление гессиана методом конечных разностей.\"\"\"\n",
    "    n = len(x)\n",
    "    hess = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            e_i = np.zeros(n)\n",
    "            e_i[i] = 1.0\n",
    "            e_j = np.zeros(n)\n",
    "            e_j[j] = 1.0\n",
    "\n",
    "            # Формула для второй производной\n",
    "            f_xx = oracle.func(x + eps*e_i + eps*e_j)\n",
    "            f_x = oracle.func(x + eps*e_i)\n",
    "            f_y = oracle.func(x + eps*e_j)\n",
    "            f_0 = oracle.func(x)\n",
    "\n",
    "            hess[i, j] = (f_xx - f_x - f_y + f_0) / (eps * eps)\n",
    "\n",
    "            if i != j:\n",
    "                hess[j, i] = hess[i, j]\n",
    "\n",
    "    return hess\n",
    "\n",
    "#1.5\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(('D:\\1Programmirovanie\\LLM универ\\4 лаба\\oracles.py'))\n",
    "\n",
    "import oracles\n",
    "from oracles import QuadraticOracle\n",
    "from optimization import gradient_descent\n",
    "\n",
    "# Создаем различные квадратичные функции\n",
    "def create_quadratic_function(condition_number=1, rotation=0):\n",
    "    \"\"\"Создает квадратичную функцию с заданным числом обусловленности.\"\"\"\n",
    "    n = 2\n",
    "\n",
    "    if condition_number == 1:\n",
    "        # Идеально обусловленная (круглые линии уровня)\n",
    "        A = np.eye(n)\n",
    "    else:\n",
    "        # Матрица с заданным числом обусловленности\n",
    "        A = np.array([[condition_number, 0], [0, 1]])\n",
    "\n",
    "        # Применяем вращение\n",
    "        if rotation != 0:\n",
    "            theta = rotation\n",
    "            R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                          [np.sin(theta), np.cos(theta)]])\n",
    "            A = R @ A @ R.T\n",
    "\n",
    "    b = np.zeros(n)\n",
    "    return QuadraticOracle(A, b)\n",
    "\n",
    "# Функция для рисования траектории\n",
    "def plot_trajectory(oracle, x0, method='gradient', ax=None, **kwargs):\n",
    "    \"\"\"Рисует траекторию метода оптимизации.\"\"\"\n",
    "    if method == 'gradient':\n",
    "        x_opt, message, history = gradient_descent(\n",
    "            oracle, x0, tolerance=1e-6, max_iter=1000, trace=True\n",
    "        )\n",
    "    # ... можно добавить другие методы\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Рисуем линии уровня\n",
    "    x_range = np.linspace(-3, 3, 100)\n",
    "    y_range = np.linspace(-3, 3, 100)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = oracle.func(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "    ax.contour(X, Y, Z, levels=20, alpha=0.5)\n",
    "\n",
    "    # Рисуем траекторию\n",
    "    traj = np.array(history['x'])\n",
    "    ax.plot(traj[:, 0], traj[:, 1], 'ro-', linewidth=2, markersize=4)\n",
    "    ax.plot(x0[0], x0[1], 'go', markersize=10, label='Start')\n",
    "    ax.plot(x_opt[0], x_opt[1], 'b*', markersize=15, label='Optimum')\n",
    "\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    return ax\n",
    "\n",
    "# Запускаем эксперимент\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Разные числа обусловленности\n",
    "condition_numbers = [1, 10, 100]\n",
    "start_points = [np.array([2.5, 2.5]), np.array([-2, 2]), np.array([2, -2])]\n",
    "\n",
    "for i, cond in enumerate(condition_numbers):\n",
    "    for j, x0 in enumerate(start_points):\n",
    "        oracle = create_quadratic_function(cond)\n",
    "        ax = axes[j, i]\n",
    "        plot_trajectory(oracle, x0, ax=ax)\n",
    "        ax.set_title(f'Condition number = {cond}, Start = {x0}')\n",
    "        ax.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#2.1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from oracles import QuadraticOracle\n",
    "from optimization import gradient_descent\n",
    "\n",
    "# Создаем различные квадратичные функции\n",
    "def create_quadratic_function(condition_number=1, rotation=0):\n",
    "    \"\"\"Создает квадратичную функцию с заданным числом обусловленности.\"\"\"\n",
    "    n = 2\n",
    "\n",
    "    if condition_number == 1:\n",
    "        # Идеально обусловленная (круглые линии уровня)\n",
    "        A = np.eye(n)\n",
    "    else:\n",
    "        # Матрица с заданным числом обусловленности\n",
    "        A = np.array([[condition_number, 0], [0, 1]])\n",
    "\n",
    "        # Применяем вращение\n",
    "        if rotation != 0:\n",
    "            theta = rotation\n",
    "            R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                          [np.sin(theta), np.cos(theta)]])\n",
    "            A = R @ A @ R.T\n",
    "\n",
    "    b = np.zeros(n)\n",
    "    return QuadraticOracle(A, b)\n",
    "\n",
    "# Функция для рисования траектории\n",
    "def plot_trajectory(oracle, x0, method='gradient', ax=None, **kwargs):\n",
    "    \"\"\"Рисует траекторию метода оптимизации.\"\"\"\n",
    "    if method == 'gradient':\n",
    "        x_opt, message, history = gradient_descent(\n",
    "            oracle, x0, tolerance=1e-6, max_iter=1000, trace=True\n",
    "        )\n",
    "    # ... можно добавить другие методы\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Рисуем линии уровня\n",
    "    x_range = np.linspace(-3, 3, 100)\n",
    "    y_range = np.linspace(-3, 3, 100)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = oracle.func(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "    ax.contour(X, Y, Z, levels=20, alpha=0.5)\n",
    "\n",
    "    # Рисуем траекторию\n",
    "    traj = np.array(history['x'])\n",
    "    ax.plot(traj[:, 0], traj[:, 1], 'ro-', linewidth=2, markersize=4)\n",
    "    ax.plot(x0[0], x0[1], 'go', markersize=10, label='Start')\n",
    "    ax.plot(x_opt[0], x_opt[1], 'b*', markersize=15, label='Optimum')\n",
    "\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    return ax\n",
    "\n",
    "# Запускаем эксперимент\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Разные числа обусловленности\n",
    "condition_numbers = [1, 10, 100]\n",
    "start_points = [np.array([2.5, 2.5]), np.array([-2, 2]), np.array([2, -2])]\n",
    "\n",
    "for i, cond in enumerate(condition_numbers):\n",
    "    for j, x0 in enumerate(start_points):\n",
    "        oracle = create_quadratic_function(cond)\n",
    "        ax = axes[j, i]\n",
    "        plot_trajectory(oracle, x0, ax=ax)\n",
    "        ax.set_title(f'Condition number = {cond}, Start = {x0}')\n",
    "        ax.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#2.2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import diags\n",
    "\n",
    "def run_condition_number_experiment(n_values=[10, 100, 1000],\n",
    "                                    kappa_values=np.logspace(0, 3, 20),\n",
    "                                    n_trials=5):\n",
    "    \"\"\"Исследует зависимость числа итераций от числа обусловленности.\"\"\"\n",
    "\n",
    "    results = {n: [] for n in n_values}\n",
    "\n",
    "    for n in n_values:\n",
    "        print(f\"Running experiments for n={n}...\")\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            iterations_vs_kappa = []\n",
    "\n",
    "            for kappa in kappa_values:\n",
    "                # Генерируем диагональную матрицу с заданным числом обусловленности\n",
    "                # Диагональные элементы от 1 до kappa\n",
    "                diag_vals = np.linspace(1, kappa, n)\n",
    "                A = diags(diag_vals, format='csr')\n",
    "                b = np.random.randn(n)\n",
    "\n",
    "                oracle = QuadraticOracle(A, b)\n",
    "                x0 = np.random.randn(n)\n",
    "\n",
    "                # Запускаем градиентный спуск\n",
    "                x_opt, message, history = gradient_descent(\n",
    "                    oracle, x0, tolerance=1e-6, max_iter=10000, trace=True\n",
    "                )\n",
    "\n",
    "                iterations_vs_kappa.append(len(history['func']))\n",
    "\n",
    "            results[n].append(iterations_vs_kappa)\n",
    "\n",
    "    # Визуализация результатов\n",
    "    colors = ['r', 'b', 'g', 'm', 'c']\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i, n in enumerate(n_values):\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        for trial_data in results[n]:\n",
    "            plt.loglog(kappa_values, trial_data, color=color, alpha=0.3, linewidth=1)\n",
    "\n",
    "        # Среднее значение\n",
    "        mean_iterations = np.mean(results[n], axis=0)\n",
    "        plt.loglog(kappa_values, mean_iterations, color=color,\n",
    "                  linewidth=3, label=f'n={n}')\n",
    "\n",
    "    plt.xlabel('Condition number (κ)', fontsize=14)\n",
    "    plt.ylabel('Number of iterations', fontsize=14)\n",
    "    plt.title('Dependence of GD iterations on condition number', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Запускаем эксперимент\n",
    "results = run_condition_number_experiment()\n",
    "\n",
    "#2.3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from oracles import create_log_reg_oracle\n",
    "from optimization import gradient_descent, newton\n",
    "\n",
    "def compare_methods_on_real_data(dataset_name='w8a'):\n",
    "    \"\"\"Сравнивает градиентный спуск и метод Ньютона на реальных данных.\"\"\"\n",
    "\n",
    "    # Загружаем данные\n",
    "    if dataset_name == 'w8a':\n",
    "        A, b = load_svmlight_file('data/w8a')\n",
    "    elif dataset_name == 'gisette':\n",
    "        A, b = load_svmlight_file('data/gisette_scale')\n",
    "    elif dataset_name == 'real-sim':\n",
    "        A, b = load_svmlight_file('data/real-sim')\n",
    "\n",
    "    # Нормализуем метки к {-1, 1}\n",
    "    b = np.where(b > 0, 1, -1)\n",
    "\n",
    "    # Параметры\n",
    "    m, n = A.shape\n",
    "    regcoef = 1.0 / m\n",
    "    x0 = np.zeros(n)\n",
    "\n",
    "    # Создаем оракул\n",
    "    oracle = create_log_reg_oracle(A, b, regcoef)\n",
    "\n",
    "    # Запускаем градиентный спуск\n",
    "    print(f\"Running Gradient Descent on {dataset_name}...\")\n",
    "    start_time = time.time()\n",
    "    x_gd, msg_gd, history_gd = gradient_descent(\n",
    "        oracle, x0, tolerance=1e-6, max_iter=1000, trace=True\n",
    "    )\n",
    "    gd_time = time.time() - start_time\n",
    "\n",
    "    # Запускаем метод Ньютона\n",
    "    print(f\"Running Newton's method on {dataset_name}...\")\n",
    "    start_time = time.time()\n",
    "    x_newton, msg_newton, history_newton = newton(\n",
    "        oracle, x0, tolerance=1e-6, max_iter=100, trace=True\n",
    "    )\n",
    "    newton_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nResults for {dataset_name}:\")\n",
    "    print(f\"GD: {msg_gd}, time: {gd_time:.2f}s\")\n",
    "    print(f\"Newton: {msg_newton}, time: {newton_time:.2f}s\")\n",
    "\n",
    "    # Визуализация\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # График 1: Значение функции vs время\n",
    "    axes[0].plot(history_gd['func'], 'b-', linewidth=2, label='Gradient Descent')\n",
    "    axes[0].plot(history_newton['func'], 'r-', linewidth=2, label=\"Newton's method\")\n",
    "    axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "    axes[0].set_ylabel('Function value', fontsize=12)\n",
    "    axes[0].set_title(f'Convergence: {dataset_name}', fontsize=14)\n",
    "    axes[0].legend(fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # График 2: Норма градиента vs время\n",
    "    grad_norm_0 = np.linalg.norm(oracle.grad(x0))\n",
    "\n",
    "    # Для градиентного спуска\n",
    "    gd_grad_norm_rel = [gn / grad_norm_0 for gn in history_gd['grad_norm']]\n",
    "\n",
    "    # Для метода Ньютона\n",
    "    newton_grad_norm_rel = [gn / grad_norm_0 for gn in history_newton['grad_norm']]\n",
    "\n",
    "    axes[1].semilogy(gd_grad_norm_rel, 'b-', linewidth=2, label='Gradient Descent')\n",
    "    axes[1].semilogy(newton_grad_norm_rel, 'r-', linewidth=2, label=\"Newton's method\")\n",
    "    axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "    axes[1].set_ylabel('Relative gradient norm', fontsize=12)\n",
    "    axes[1].set_title(f'Gradient norm reduction: {dataset_name}', fontsize=14)\n",
    "    axes[1].legend(fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Анализ сложности\n",
    "    print(\"\\nComplexity analysis:\")\n",
    "    print(f\"Data dimensions: m={m}, n={n}\")\n",
    "    print(f\"\\nGradient Descent per iteration:\")\n",
    "    print(f\"  - Computation: O(m*n) for Ax\")\n",
    "    print(f\"  - Memory: O(m + n)\")\n",
    "\n",
    "    print(f\"\\nNewton's method per iteration:\")\n",
    "    print(f\"  - Computation: O(m*n^2 + n^3) for Hessian and solve\")\n",
    "    print(f\"  - Memory: O(n^2) for Hessian\")\n",
    "\n",
    "    return history_gd, history_newton\n",
    "\n",
    "# Запускаем сравнение для разных датасетов\n",
    "datasets = ['w8a', 'gisette', 'real-sim']\n",
    "for dataset in datasets:\n",
    "    try:\n",
    "        compare_methods_on_real_data(dataset)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Dataset {dataset} not found. Skipping...\")\n",
    "\n"
   ],
   "id": "563bff57881b08c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:330: SyntaxWarning: invalid escape sequence '\\L'\n",
      "<>:330: SyntaxWarning: invalid escape sequence '\\L'\n",
      "C:\\Users\\Nikolaii\\AppData\\Local\\Temp\\ipykernel_8000\\140941779.py:330: SyntaxWarning: invalid escape sequence '\\L'\n",
      "  sys.path.append(('D:\\1Programmirovanie\\LLM универ\\4 лаба\\oracles.py'))\n",
      "C:\\Users\\Nikolaii\\AppData\\Local\\Temp\\ipykernel_8000\\140941779.py:330: SyntaxWarning: invalid escape sequence '\\L'\n",
      "  sys.path.append(('D:\\1Programmirovanie\\LLM универ\\4 лаба\\oracles.py'))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'oracles'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 332\u001B[39m\n\u001B[32m    328\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\n\u001B[32m    330\u001B[39m sys.path.append((\u001B[33m'\u001B[39m\u001B[33mD:\u001B[39m\u001B[38;5;130;01m\\1\u001B[39;00m\u001B[33mProgrammirovanie\u001B[39m\u001B[33m\\\u001B[39m\u001B[33mLLM универ\u001B[39m\u001B[38;5;130;01m\\4\u001B[39;00m\u001B[33m лаба\u001B[39m\u001B[33m\\\u001B[39m\u001B[33moracles.py\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m--> \u001B[39m\u001B[32m332\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01moracles\u001B[39;00m\n\u001B[32m    333\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01moracles\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m QuadraticOracle\n\u001B[32m    334\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01moptimization\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m gradient_descent\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'oracles'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3064af061ae72cb4"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
