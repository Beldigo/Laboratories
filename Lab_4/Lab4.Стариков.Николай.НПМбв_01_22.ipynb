{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1e7637b1-2fcc-48ad-8650-934f918e9b4d",
      "cell_type": "code",
      "source": "#1 Реализация LineSearchTool\nimport numpy as np\nfrom numpy.linalg import LinAlgError\nimport scipy\nfrom scipy.optimize import linesearch\nfrom datetime import datetime\nfrom collections import defaultdict\n\n\nclass LineSearchTool(object):\n   \n    def __init__(self, method='Wolfe', **kwargs):\n        self._method = method\n        if self._method == 'Wolfe':\n            self.c1 = kwargs.get('c1', 1e-4)\n            self.c2 = kwargs.get('c2', 0.9)\n            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n        elif self._method == 'Armijo':\n            self.c1 = kwargs.get('c1', 1e-4)\n            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n        elif self._method == 'Constant':\n            self.c = kwargs.get('c', 1.0)\n        else:\n            raise ValueError('Unknown method {}'.format(method))\n\n    @classmethod\n    def from_dict(cls, options):\n        if type(options) != dict:\n            raise TypeError('LineSearchTool initializer must be of type dict')\n        return cls(**options)\n\n    def to_dict(self):\n        return self.__dict__\n\n    def line_search(self, oracle, x_k, d_k, previous_alpha=None):\n \n        phi0 = oracle.func_directional(x_k, d_k, 0)\n        dphi0 = oracle.grad_directional(x_k, d_k, 0)\n        \n        # Проверка, что направление является направлением спуска\n        if dphi0 >= 0:\n            return None\n        \n        if self._method == 'Constant':\n            return self.c\n        \n        elif self._method == 'Armijo':\n            # Адаптивный метод подбора шага\n            if previous_alpha is not None:\n                alpha = previous_alpha * 2\n            else:\n                alpha = self.alpha_0\n                \n            # Метод дробления шага (бэктрекинг)\n            while oracle.func_directional(x_k, d_k, alpha) > phi0 + self.c1 * alpha * dphi0:\n                alpha *= 0.5\n                if alpha < 1e-12:  # Предотвращение бесконечного цикла\n                    return None\n            return alpha\n        \n        elif self._method == 'Wolfe':\n            # Используем библиотечную реализацию Wolfe условий\n            try:\n                alpha, _, _, _ = linesearch.scalar_search_wolfe2(\n                    phi=lambda alpha: oracle.func_directional(x_k, d_k, alpha),\n                    derphi=lambda alpha: oracle.grad_directional(x_k, d_k, alpha),\n                    phi0=phi0,\n                    derphi0=dphi0,\n                    c1=self.c1,\n                    c2=self.c2\n                )\n                \n                if alpha is None:\n                    # Если Wolfe не сработал, используем Armijo с alpha_0\n                    alpha = self.alpha_0\n                    while oracle.func_directional(x_k, d_k, alpha) > phi0 + self.c1 * alpha * dphi0:\n                        alpha *= 0.5\n                        if alpha < 1e-12:\n                            return None\n                return alpha\n            except Exception:\n                # В случае ошибки используем Armijo\n                alpha = self.alpha_0\n                while oracle.func_directional(x_k, d_k, alpha) > phi0 + self.c1 * alpha * dphi0:\n                    alpha *= 0.5\n                    if alpha < 1e-12:\n                        return None\n                return alpha\n\n#2 Реализация Gradient Descent\ndef get_line_search_tool(line_search_options=None):\n    if line_search_options:\n        if type(line_search_options) is LineSearchTool:\n            return line_search_options\n        else:\n            return LineSearchTool.from_dict(line_search_options)\n    else:\n        return LineSearchTool()\n\n\ndef gradient_descent(oracle, x_0, tolerance=1e-5, max_iter=10000,\n                     line_search_options=None, trace=False, display=False):\n    \n    history = defaultdict(list) if trace else None\n    line_search_tool = get_line_search_tool(line_search_options)\n    x_k = np.copy(x_0)\n    \n    # Вычисляем начальный градиент и его норму для критерия остановки\n    grad_k = oracle.grad(x_k)\n    grad_norm_2 = np.linalg.norm(grad_k) ** 2\n    grad_norm_0_2 = grad_norm_2\n    \n    if trace:\n        history['time'].append(0.0)\n        history['func'].append(oracle.func(x_k))\n        history['grad_norm'].append(np.sqrt(grad_norm_2))\n        if x_k.size <= 2:\n            history['x'].append(x_k.copy())\n    \n    start_time = datetime.now()\n    previous_alpha = None\n    \n    for iteration in range(max_iter):\n        # Критерий остановки (1.1): ||∇f(x_k)||^2 ≤ ε * ||∇f(x_0)||^2\n        if grad_norm_2 <= tolerance * grad_norm_0_2:\n            return x_k, 'success', history\n        \n        # Направление спуска - антиградиент\n        d_k = -grad_k\n        \n        # Линейный поиск\n        try:\n            alpha = line_search_tool.line_search(oracle, x_k, d_k, previous_alpha)\n            if alpha is None:\n                if display:\n                    print(f\"Iteration {iteration}: line search failed\")\n                return x_k, 'computational_error', history\n            previous_alpha = alpha\n        except Exception as e:\n            if display:\n                print(f\"Iteration {iteration}: computational error - {e}\")\n            return x_k, 'computational_error', history\n        \n        # Обновление точки\n        x_k = x_k + alpha * d_k\n        \n        # Вычисление градиента в новой точке\n        try:\n            grad_k = oracle.grad(x_k)\n            grad_norm_2 = np.linalg.norm(grad_k) ** 2\n        except Exception as e:\n            if display:\n                print(f\"Iteration {iteration}: computational error in gradient - {e}\")\n            return x_k, 'computational_error', history\n        \n        # Сохранение информации для трассировки\n        if trace:\n            current_time = (datetime.now() - start_time).total_seconds()\n            history['time'].append(current_time)\n            history['func'].append(oracle.func(x_k))\n            history['grad_norm'].append(np.sqrt(grad_norm_2))\n            if x_k.size <= 2:\n                history['x'].append(x_k.copy())\n        \n        if display and (iteration + 1) % 100 == 0:\n            print(f\"Iteration {iteration + 1}: f = {history['func'][-1]:.6f}, \"\n                  f\"||grad||^2 = {grad_norm_2:.6e}, alpha = {alpha:.6f}\")\n    \n    # Превышено максимальное число итераций\n    return x_k, 'iterations_exceeded', history\n\n#3 Реализация Newton Method\ndef newton(oracle, x_0, tolerance=1e-5, max_iter=100,\n           line_search_options=None, trace=False, display=False):\n    \n    history = defaultdict(list) if trace else None\n    line_search_tool = get_line_search_tool(line_search_options)\n    x_k = np.copy(x_0)\n    \n    # Вычисляем начальный градиент и его норму для критерия остановки\n    try:\n        grad_k = oracle.grad(x_k)\n        grad_norm_2 = np.linalg.norm(grad_k) ** 2\n        grad_norm_0_2 = grad_norm_2\n    except Exception as e:\n        if display:\n            print(f\"Initial point: computational error in gradient - {e}\")\n        return x_k, 'computational_error', history\n    \n    if trace:\n        history['time'].append(0.0)\n        history['func'].append(oracle.func(x_k))\n        history['grad_norm'].append(np.sqrt(grad_norm_2))\n        if x_k.size <= 2:\n            history['x'].append(x_k.copy())\n    \n    start_time = datetime.now()\n    \n    for iteration in range(max_iter):\n        # Критерий остановки (1.1): ||∇f(x_k)||^2 ≤ ε * ||∇f(x_0)||^2\n        if grad_norm_2 <= tolerance * grad_norm_0_2:\n            return x_k, 'success', history\n        \n        # Вычисление гессиана и решение системы ∇^2 f(x_k) d_k = -∇f(x_k)\n        try:\n            hess_k = oracle.hess(x_k)\n            # Используем разложение Холецкого для положительно определенных матриц\n            try:\n                cho_factor = scipy.linalg.cho_factor(hess_k)\n                d_k = scipy.linalg.cho_solve(cho_factor, -grad_k)\n            except LinAlgError:\n                # Если матрица не положительно определена, используем общий метод\n                d_k = np.linalg.solve(hess_k, -grad_k)\n        except Exception as e:\n            if display:\n                print(f\"Iteration {iteration}: Newton direction error - {e}\")\n            return x_k, 'newton_direction_error', history\n        \n        # Проверка направления спуска\n        dphi0 = grad_k.dot(d_k)\n        if dphi0 >= 0:\n            if display:\n                print(f\"Iteration {iteration}: Not a descent direction\")\n            return x_k, 'newton_direction_error', history\n        \n        # Линейный поиск (всегда начинаем с alpha = 1.0 для метода Ньютона)\n        try:\n            # Для метода Ньютона всегда пробуем alpha = 1.0 первым\n            if line_search_tool._method == 'Wolfe':\n                # Специальная обработка для Wolfe: сначала пробуем alpha=1.0\n                phi0 = oracle.func_directional(x_k, d_k, 0)\n                dphi0 = oracle.grad_directional(x_k, d_k, 0)\n                \n                try:\n                    alpha, _, _, _ = linesearch.scalar_search_wolfe2(\n                        phi=lambda alpha: oracle.func_directional(x_k, d_k, alpha),\n                        derphi=lambda alpha: oracle.grad_directional(x_k, d_k, alpha),\n                        phi0=phi0,\n                        derphi0=dphi0,\n                        c1=line_search_tool.c1,\n                        c2=line_search_tool.c2,\n                        amax=1.0  # Ограничиваем максимальный шаг 1.0\n                    )\n                    \n                    if alpha is None:\n                        # Если Wolfe не сработал, используем Armijo с alpha=1.0\n                        alpha = 1.0\n                        while oracle.func_directional(x_k, d_k, alpha) > phi0 + line_search_tool.c1 * alpha * dphi0:\n                            alpha *= 0.5\n                            if alpha < 1e-12:\n                                alpha = None\n                                break\n                except Exception:\n                    alpha = 1.0\n                    while oracle.func_directional(x_k, d_k, alpha) > phi0 + line_search_tool.c1 * alpha * dphi0:\n                        alpha *= 0.5\n                        if alpha < 1e-12:\n                            alpha = None\n                            break\n            else:\n                # Для Armijo и Constant используем стандартную процедуру\n                alpha = line_search_tool.line_search(oracle, x_k, d_k, previous_alpha=1.0)\n            \n            if alpha is None:\n                if display:\n                    print(f\"Iteration {iteration}: line search failed\")\n                return x_k, 'computational_error', history\n        except Exception as e:\n            if display:\n                print(f\"Iteration {iteration}: computational error in line search - {e}\")\n            return x_k, 'computational_error', history\n        \n        # Обновление точки\n        x_k = x_k + alpha * d_k\n        \n        # Вычисление градиента в новой точке\n        try:\n            grad_k = oracle.grad(x_k)\n            grad_norm_2 = np.linalg.norm(grad_k) ** 2\n        except Exception as e:\n            if display:\n                print(f\"Iteration {iteration}: computational error in gradient - {e}\")\n            return x_k, 'computational_error', history\n        \n        # Сохранение информации для трассировки\n        if trace:\n            current_time = (datetime.now() - start_time).total_seconds()\n            history['time'].append(current_time)\n            history['func'].append(oracle.func(x_k))\n            history['grad_norm'].append(np.sqrt(grad_norm_2))\n            if x_k.size <= 2:\n                history['x'].append(x_k.copy())\n        \n        if display and (iteration + 1) % 10 == 0:\n            print(f\"Iteration {iteration + 1}: f = {history['func'][-1]:.6f}, \"\n                  f\"||grad||^2 = {grad_norm_2:.6e}, alpha = {alpha:.6f}\")\n    \n    # Превышено максимальное число итераций\n    return x_k, 'iterations_exceeded', history\n\n#4 Реализация LogRegL2Oracle\n\nimport numpy as np\nimport scipy\nfrom scipy.special import expit\n\n\nclass BaseSmoothOracle(object):\n    \"\"\"\n    Base class for implementation of oracles.\n    \"\"\"\n    def func(self, x):\n        \"\"\"\n        Computes the value of function at point x.\n        \"\"\"\n        raise NotImplementedError('Func oracle is not implemented.')\n\n    def grad(self, x):\n        \"\"\"\n        Computes the gradient at point x.\n        \"\"\"\n        raise NotImplementedError('Grad oracle is not implemented.')\n    \n    def hess(self, x):\n        \"\"\"\n        Computes the Hessian matrix at point x.\n        \"\"\"\n        raise NotImplementedError('Hessian oracle is not implemented.')\n    \n    def func_directional(self, x, d, alpha):\n        \"\"\"\n        Computes phi(alpha) = f(x + alpha*d).\n        \"\"\"\n        return np.squeeze(self.func(x + alpha * d))\n\n    def grad_directional(self, x, d, alpha):\n        \"\"\"\n        Computes phi'(alpha) = (f(x + alpha*d))'_{alpha}\n        \"\"\"\n        return np.squeeze(self.grad(x + alpha * d).dot(d))\n\n\nclass QuadraticOracle(BaseSmoothOracle):\n    \"\"\"\n    Oracle for quadratic function:\n       func(x) = 1/2 x^TAx - b^Tx.\n    \"\"\"\n\n    def __init__(self, A, b):\n        if not scipy.sparse.isspmatrix_dia(A) and not np.allclose(A, A.T):\n            raise ValueError('A should be a symmetric matrix.')\n        self.A = A\n        self.b = b\n\n    def func(self, x):\n        return 0.5 * np.dot(self.A.dot(x), x) - self.b.dot(x)\n\n    def grad(self, x):\n        return self.A.dot(x) - self.b\n\n    def hess(self, x):\n        return self.A \n\n\nclass LogRegL2Oracle(BaseSmoothOracle):\n    \n    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):\n        self.matvec_Ax = matvec_Ax\n        self.matvec_ATx = matvec_ATx\n        self.matmat_ATsA = matmat_ATsA\n        self.b = b\n        self.regcoef = regcoef\n        self.m = len(b)\n\n    def func(self, x):\n        \"\"\"\n        f(x) = 1/m * sum(log(1 + exp(-b_i * a_i^T x))) + regcoef/2 * ||x||^2\n        Векторизованная реализация без циклов.\n        \"\"\"\n        Ax = self.matvec_Ax(x)\n        # Используем np.logaddexp для численной устойчивости\n        # log(1 + exp(-b_i * a_i^T x)) = -logaddexp(0, -b_i * a_i^T x)\n        # Но лучше直接用: logaddexp(0, -b_i * a_i^T x)\n        log_term = np.logaddexp(0, -self.b * Ax)\n        f = np.mean(log_term) + 0.5 * self.regcoef * np.dot(x, x)\n        return f\n\n    def grad(self, x):\n        \"\"\"\n        ∇f(x) = -1/m * A^T (b / (1 + exp(b * Ax))) + regcoef * x\n        Используем scipy.special.expit для сигмоиды.\n        \"\"\"\n        Ax = self.matvec_Ax(x)\n        # sigma = 1/(1+exp(-z))\n        # b / (1 + exp(b * Ax)) = b * sigma(-b * Ax)\n        sigma_minus = expit(-self.b * Ax)\n        grad_main = -self.matvec_ATx(self.b * sigma_minus) / self.m\n        grad_reg = self.regcoef * x\n        return grad_main + grad_reg\n\n    def hess(self, x):\n        \"\"\"\n        ∇^2 f(x) = 1/m * A^T * diag(sigma(b*Ax) * sigma(-b*Ax)) * A + regcoef * I\n        \"\"\"\n        Ax = self.matvec_Ax(x)\n        # sigma(b*Ax) * sigma(-b*Ax) - это variance для логистической регрессии\n        sigma_pos = expit(self.b * Ax)\n        sigma_neg = expit(-self.b * Ax)\n        s = sigma_pos * sigma_neg  # Поэлементное произведение\n        \n        # A^T * Diag(s) * A\n        hess_main = self.matmat_ATsA(s) / self.m\n        hess_reg = self.regcoef * np.eye(len(x))\n        return hess_main + hess_reg\n\n\nclass LogRegL2OptimizedOracle(LogRegL2Oracle):\n    \n    def __init__(self, matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef):\n        super().__init__(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)\n        self._cached_Ax = None\n        self._cached_Ad = None\n        self._cached_x = None\n        self._cached_d = None\n        self._cached_x_alpha = None\n        self._cached_Ax_alpha = None\n\n    def func_directional(self, x, d, alpha):\n        \"\"\"\n        Оптимизированная версия: предвычисляем Ax и Ad.\n        \"\"\"\n        x_alpha = x + alpha * d\n        \n        # Проверяем кэш для Ax\n        if self._cached_x is not None and np.array_equal(x, self._cached_x):\n            Ax = self._cached_Ax\n        else:\n            Ax = self.matvec_Ax(x)\n            self._cached_x = x.copy()\n            self._cached_Ax = Ax\n        \n        # Вычисляем Ad, если нужно\n        if self._cached_d is not None and np.array_equal(d, self._cached_d):\n            Ad = self._cached_Ad\n        else:\n            Ad = self.matvec_Ax(d)\n            self._cached_d = d.copy()\n            self._cached_Ad = Ad\n        \n        # Вычисляем Ax_alpha = Ax + alpha * Ad\n        Ax_alpha = Ax + alpha * Ad\n        \n        # Кэшируем для последующих вызовов в этой точке\n        self._cached_x_alpha = x_alpha.copy()\n        self._cached_Ax_alpha = Ax_alpha\n        \n        # Вычисляем значение функции\n        log_term = np.logaddexp(0, -self.b * Ax_alpha)\n        f = np.mean(log_term) + 0.5 * self.regcoef * np.dot(x_alpha, x_alpha)\n        return f\n\n    def grad_directional(self, x, d, alpha):\n        \"\"\"\n        Оптимизированная версия: предвычисляем Ax и Ad.\n        \"\"\"\n        x_alpha = x + alpha * d\n        \n        # Проверяем кэш для Ax_alpha\n        if (self._cached_x_alpha is not None and \n            np.array_equal(x_alpha, self._cached_x_alpha)):\n            Ax_alpha = self._cached_Ax_alpha\n        else:\n            # Проверяем кэш для Ax\n            if self._cached_x is not None and np.array_equal(x, self._cached_x):\n                Ax = self._cached_Ax\n            else:\n                Ax = self.matvec_Ax(x)\n                self._cached_x = x.copy()\n                self._cached_Ax = Ax\n            \n            # Проверяем кэш для Ad\n            if self._cached_d is not None and np.array_equal(d, self._cached_d):\n                Ad = self._cached_Ad\n            else:\n                Ad = self.matvec_Ax(d)\n                self._cached_d = d.copy()\n                self._cached_Ad = Ad\n            \n            Ax_alpha = Ax + alpha * Ad\n            self._cached_x_alpha = x_alpha.copy()\n            self._cached_Ax_alpha = Ax_alpha\n        \n        # Вычисляем производную\n        sigma_minus = expit(-self.b * Ax_alpha)\n        grad_dir = -np.dot(self.b * sigma_minus, Ad) / self.m + self.regcoef * np.dot(x_alpha, d)\n        return grad_dir\n\n\ndef create_log_reg_oracle(A, b, regcoef, oracle_type='usual'):\n    \n    def matvec_Ax(x):\n        return A.dot(x)\n    \n    def matvec_ATx(x):\n        return A.T.dot(x)\n\n    def matmat_ATsA(s):\n        # A^T * Diag(s) * A\n        # Для разреженных матриц это может быть оптимизировано\n        if scipy.sparse.issparse(A):\n            # Для разреженных матриц: A^T * (s.reshape(-1, 1) * A)\n            # Это более эффективно, чем явное создание диагональной матрицы\n            s_diag = scipy.sparse.diags(s)\n            return A.T.dot(s_diag.dot(A))\n        else:\n            # Для плотных матриц\n            return A.T @ (s[:, np.newaxis] * A)\n\n    if oracle_type == 'usual':\n        oracle = LogRegL2Oracle\n    elif oracle_type == 'optimized':\n        oracle = LogRegL2OptimizedOracle\n    else:\n        raise ValueError('Unknown oracle_type=%s' % oracle_type)\n    \n    return oracle(matvec_Ax, matvec_ATx, matmat_ATsA, b, regcoef)\n\n#5 Реализация конечных разностей \n\ndef grad_finite_diff(func, x, eps=1e-8):\n    \n    n = len(x)\n    grad = np.zeros(n)\n    f_x = func(x)\n    \n    for i in range(n):\n        x_plus_eps = x.copy()\n        x_plus_eps[i] += eps\n        grad[i] = (func(x_plus_eps) - f_x) / eps\n    \n    return grad\n\n\ndef hess_finite_diff(func, x, eps=1e-5):\n   \n    n = len(x)\n    hess = np.zeros((n, n))\n    f_x = func(x)\n    \n    for i in range(n):\n        x_plus_eps_i = x.copy()\n        x_plus_eps_i[i] += eps\n        f_x_plus_eps_i = func(x_plus_eps_i)\n        \n        for j in range(i, n):\n            x_plus_eps_ij = x.copy()\n            x_plus_eps_ij[i] += eps\n            x_plus_eps_ij[j] += eps\n            f_x_plus_eps_ij = func(x_plus_eps_ij)\n            \n            x_plus_eps_j = x.copy()\n            x_plus_eps_j[j] += eps\n            f_x_plus_eps_j = func(x_plus_eps_j)\n            \n            hess_ij = (f_x_plus_eps_ij - f_x_plus_eps_i - f_x_plus_eps_j + f_x) / (eps ** 2)\n            hess[i, j] = hess_ij\n            hess[j, i] = hess_ij\n    \n    return hess\n\n\n\n# ## 1. Траектория градиентного спуска на квадратичной функции\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.sparse\nimport time\nfrom datetime import datetime\nfrom sklearn.datasets import load_svmlight_file\n\nimport optimization\nimport oracles\nfrom plot_trajectory_2d import plot_levels, plot_trajectory\n\n\n# Функция для генерации квадратичной задачи с заданным числом обусловленности\ndef generate_quadratic_problem(n, kappa, seed=42):\n    \"\"\"\n    Генерирует квадратичную задачу f(x) = 1/2 x^T A x - b^T x\n    с числом обусловленности kappa.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # Генерируем диагональные элементы в диапазоне [1, kappa]\n    diag = np.random.uniform(1, kappa, n)\n    # Упорядочиваем, чтобы min = 1, max = kappa\n    diag = np.sort(diag)\n    diag[0] = 1.0\n    diag[-1] = kappa\n    \n    # Создаем диагональную матрицу\n    A = scipy.sparse.diags(diag, format='dia')\n    \n    # Генерируем случайный вектор b\n    b = np.random.randn(n)\n    \n    return A, b\n\n\n# Эксперимент 1: Траектория градиентного спуска на квадратичных функциях\n\n# Функция 1: Хорошо обусловленная (kappa = 2)\nA1, b1 = generate_quadratic_problem(2, 2, seed=1)\noracle1 = oracles.QuadraticOracle(A1, b1)\n\n# Функция 2: Плохо обусловленная (kappa = 100)\nA2, b2 = generate_quadratic_problem(2, 100, seed=1)\noracle2 = oracles.QuadraticOracle(A2, b2)\n\n# Функция 3: Очень плохо обусловленная (kappa = 1000)\nA3, b3 = generate_quadratic_problem(2, 1000, seed=1)\noracle3 = oracles.QuadraticOracle(A3, b3)\n\n# Запускаем градиентный спуск с разными стратегиями выбора шага\nx0 = np.array([3.0, 2.0])\ntolerance = 1e-6\n\n# Для хорошо обусловленной функции\nprint(\"Хорошо обусловленная функция (kappa=2):\")\nmethods = [\n    {'method': 'Constant', 'c': 0.1},\n    {'method': 'Constant', 'c': 0.5},\n    {'method': 'Armijo', 'c1': 1e-4, 'alpha_0': 1.0},\n    {'method': 'Wolfe', 'c1': 1e-4, 'c2': 0.9}\n]\n\nresults1 = {}\nfor ls_options in methods:\n    name = f\"{ls_options['method']}\"\n    if 'c' in ls_options:\n        name += f\" c={ls_options['c']}\"\n    print(f\"  {name}...\")\n    x_star, msg, history = optimization.gradient_descent(\n        oracle1, x0, tolerance=tolerance, max_iter=1000,\n        line_search_options=ls_options, trace=True, display=False\n    )\n    results1[name] = {'x_star': x_star, 'msg': msg, 'history': history}\n    print(f\"    {msg}, итераций: {len(history['func'])-1}\")\n\n# Визуализация траекторий\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Функция 1: хорошо обусловленная\nax = axes[0, 0]\nplot_levels(oracle1.func, xrange=[-2, 4], yrange=[-1, 3], levels=[0, 0.5, 2, 5, 10, 20])\nfor name, result in results1.items():\n    if result['history'] is not None and 'x' in result['history']:\n        plot_trajectory(oracle1.func, result['history']['x'], label=name)\nax.set_title(f'Хорошо обусловленная (kappa=2), итераций: {[len(r[\"history\"][\"func\"])-1 for r in results1.values()]}')\nax.legend()\nax.grid(True)\n\n# Функция 2: плохо обусловленная\nax = axes[0, 1]\nplot_levels(oracle2.func, xrange=[-2, 4], yrange=[-1, 3], levels=[0, 0.5, 2, 5, 10, 20])\nresults2 = {}\nfor ls_options in methods:\n    name = f\"{ls_options['method']}\"\n    if 'c' in ls_options:\n        name += f\" c={ls_options['c']}\"\n    x_star, msg, history = optimization.gradient_descent(\n        oracle2, x0, tolerance=tolerance, max_iter=1000,\n        line_search_options=ls_options, trace=True, display=False\n    )\n    results2[name] = {'x_star': x_star, 'msg': msg, 'history': history}\n    if history is not None and 'x' in history:\n        plot_trajectory(oracle2.func, history['x'], label=name)\nax.set_title(f'Плохо обусловленная (kappa=100), итераций: {[len(r[\"history\"][\"func\"])-1 for r in results2.values()]}')\nax.legend()\nax.grid(True)\n\n# Функция 3: очень плохо обусловленная\nax = axes[1, 0]\nplot_levels(oracle3.func, xrange=[-2, 4], yrange=[-1, 3], levels=[0, 0.5, 2, 5, 10, 20])\nresults3 = {}\nfor ls_options in methods:\n    name = f\"{ls_options['method']}\"\n    if 'c' in ls_options:\n        name += f\" c={ls_options['c']}\"\n    x_star, msg, history = optimization.gradient_descent(\n        oracle3, x0, tolerance=tolerance, max_iter=1000,\n        line_search_options=ls_options, trace=True, display=False\n    )\n    results3[name] = {'x_star': x_star, 'msg': msg, 'history': history}\n    if history is not None and 'x' in history:\n        plot_trajectory(oracle3.func, history['x'], label=name)\nax.set_title(f'Очень плохо обусловленная (kappa=1000), итераций: {[len(r[\"history\"][\"func\"])-1 for r in results3.values()]}')\nax.legend()\nax.grid(True)\n\n# Сходимость по функции для разных стратегий на плохо обусловленной функции\nax = axes[1, 1]\nfor name, result in results2.items():\n    if result['history'] is not None:\n        iterations = range(len(result['history']['func']))\n        ax.semilogy(iterations, result['history']['func'], label=name, linewidth=2)\nax.set_xlabel('Итерация')\nax.set_ylabel('f(x_k)')\nax.set_title('Сходимость по функции (kappa=100)')\nax.legend()\nax.grid(True)\n\nplt.tight_layout()\nplt.savefig('trajectory_experiment.png', dpi=150)\nplt.show()\n\n# Выводы по эксперименту 1\nprint(\"\"\"\nВыводы по эксперименту 1:\n1. Число обусловленности сильно влияет на скорость сходимости градиентного спуска.\n   Чем хуже обусловленность, тем больше итераций требуется для сходимости.\n2. Константный шаг критически зависит от выбора параметра. Слишком большой шаг\n   приводит к расходимости, слишком маленький - к медленной сходимости.\n3. Адаптивные методы (Armijo, Wolfe) автоматически подбирают подходящий шаг и\n   работают хорошо для всех функций.\n4. Wolfe условия обычно дают более быструю сходимость, чем Armijo, но требуют\n   больше вычислений на итерацию.\n\"\"\")\n\n# ## 2. Зависимость числа итераций от числа обусловленности и размерности\n\ndef run_conditioning_experiment():\n    \"\"\"\n    Эксперимент по исследованию зависимости числа итераций от числа обусловленности\n    и размерности пространства.\n    \"\"\"\n    np.random.seed(42)\n    \n    # Размерности для тестирования\n    dimensions = [10, 100, 500, 1000]\n    # Числа обусловленности\n    kappas = np.logspace(1, 4, 10)  # от 10 до 10000\n    \n    # Параметры эксперимента\n    tolerance = 1e-6\n    n_trials = 5  # число запусков для каждого параметра\n    \n    results = {}\n    colors = ['red', 'blue', 'green', 'orange']\n    \n    for dim_idx, n in enumerate(dimensions):\n        print(f\"\\nРазмерность n = {n}\")\n        color = colors[dim_idx % len(colors)]\n        \n        iterations_matrix = []\n        \n        for kappa in kappas:\n            kappa_iterations = []\n            \n            for trial in range(n_trials):\n                # Генерируем задачу\n                A, b = generate_quadratic_problem(n, kappa, seed=42 + trial * 100 + dim_idx * 10)\n                oracle = oracles.QuadraticOracle(A, b)\n                \n                # Начальная точка\n                x0 = np.random.randn(n)\n                \n                # Запускаем градиентный спуск с Armijo правилом\n                x_star, msg, history = optimization.gradient_descent(\n                    oracle, x0, tolerance=tolerance, max_iter=10000,\n                    line_search_options={'method': 'Armijo', 'c1': 1e-4, 'alpha_0': 1.0},\n                    trace=False, display=False\n                )\n                \n                if msg == 'success':\n                    iterations = len(history['func']) - 1 if history else 0\n                else:\n                    iterations = 10000  # максимум\n                \n                kappa_iterations.append(iterations)\n                print(f\"  kappa = {kappa:.1f}, trial {trial+1}: {iterations} итераций\")\n            \n            iterations_matrix.append(kappa_iterations)\n        \n        results[n] = {'kappas': kappas, 'iterations': iterations_matrix, 'color': color}\n    \n    return results\n\n# Запускаем эксперимент (может занять много времени)\n# Для демонстрации используем меньшие размерности\nresults = run_conditioning_experiment()\n\n# Визуализация результатов\nplt.figure(figsize=(12, 8))\n\nfor n, data in results.items():\n    kappas = data['kappas']\n    iterations_matrix = np.array(data['iterations'])\n    color = data['color']\n    \n    # Рисуем все кривые для данного n\n    for trial in range(iterations_matrix.shape[1]):\n        plt.loglog(kappas, iterations_matrix[:, trial], \n                   color=color, alpha=0.3, linewidth=1)\n    \n    # Рисуем среднее значение жирной линией\n    mean_iterations = np.mean(iterations_matrix, axis=1)\n    plt.loglog(kappas, mean_iterations, color=color, linewidth=3, \n               label=f'n = {n} (среднее)')\n\nplt.xlabel('Число обусловленности κ')\nplt.ylabel('Число итераций до сходимости')\nplt.title('Зависимость числа итераций градиентного спуска от κ и n')\nplt.grid(True, which='both', alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.savefig('conditioning_experiment.png', dpi=150)\nplt.show()\n\nprint(\"\"\"\nВыводы по эксперименту 2:\n1. Число итераций градиентного спуска линейно растет с числом обусловленности κ.\n   Это соответствует теоретической оценке O(κ log(1/ε)).\n2. Размерность n практически не влияет на число итераций для квадратичных задач\n   с диагональной матрицей Гессе, так как в этом случае градиентный спуск\n   сводится к независимой оптимизации по каждой координате.\n3. Разброс результатов обусловлен случайной генерацией матрицы A и начальной точки.\n4. Адаптивные методы выбора шага (Armijo) позволяют достичь теоретической\n   линейной зависимости от κ без подбора параметров.\n\"\"\")\n\n# ## 3. Сравнение градиентного спуска и метода Ньютона на задаче логистической регрессии\n\ndef load_libsvm_data(dataset_name):\n    \"\"\"\n    Загрузка данных с сайта LIBSVM.\n    Для демонстрации используем локальные файлы или генерируем синтетические данные.\n    \"\"\"\n    try:\n        if dataset_name == 'w8a':\n            X, y = load_svmlight_file('data/w8a')\n        elif dataset_name == 'gisette':\n            X, y = load_svmlight_file('data/gisette_scale')\n        elif dataset_name == 'real-sim':\n            X, y = load_svmlight_file('data/real-sim')\n        else:\n            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n        y = 2 * (y > 0) - 1  # Преобразуем в {-1, 1}\n        return X, y\n    except:\n        # Если файлы не найдены, генерируем синтетические данные\n        print(f\"Датасет {dataset_name} не найден, генерируем синтетические данные\")\n        np.random.seed(42)\n        m, n = 1000, 100\n        X = np.random.randn(m, n)\n        y = np.sign(np.random.randn(m))\n        return X, y\n\ndef run_logreg_comparison(dataset_name):\n    \"\"\"\n    Сравнение градиентного спуска и метода Ньютона на задаче логистической регрессии.\n    \"\"\"\n    print(f\"\\n=== Сравнение на датасете {dataset_name} ===\")\n    \n    # Загрузка данных\n    X, y = load_libsvm_data(dataset_name)\n    m, n = X.shape\n    regcoef = 1.0 / m\n    \n    print(f\"Размерность: m={m}, n={n}\")\n    print(f\"Коэффициент регуляризации: {regcoef:.6f}\")\n    \n    # Создание оракула\n    oracle = oracles.create_log_reg_oracle(X, y, regcoef, oracle_type='usual')\n    \n    # Начальная точка\n    x0 = np.zeros(n)\n    tolerance = 1e-6\n    \n    # Градиентный спуск\n    print(\"\\nЗапуск градиентного спуска...\")\n    start_time = time.time()\n    x_gd, msg_gd, history_gd = optimization.gradient_descent(\n        oracle, x0, tolerance=tolerance, max_iter=1000,\n        line_search_options={'method': 'Armijo', 'c1': 1e-4, 'alpha_0': 1.0},\n        trace=True, display=False\n    )\n    gd_time = time.time() - start_time\n    print(f\"Градиентный спуск: {msg_gd}, итераций: {len(history_gd['func'])-1}, время: {gd_time:.2f}с\")\n    \n    # Метод Ньютона\n    print(\"\\nЗапуск метода Ньютона...\")\n    start_time = time.time()\n    x_newton, msg_newton, history_newton = optimization.newton(\n        oracle, x0, tolerance=tolerance, max_iter=100,\n        line_search_options={'method': 'Wolfe', 'c1': 1e-4, 'c2': 0.9},\n        trace=True, display=False\n    )\n    newton_time = time.time() - start_time\n    print(f\"Метод Ньютона: {msg_newton}, итераций: {len(history_newton['func'])-1}, время: {newton_time:.2f}с\")\n    \n    return {\n        'dataset': dataset_name,\n        'm': m, 'n': n,\n        'gd': {'history': history_gd, 'time': gd_time, 'msg': msg_gd},\n        'newton': {'history': history_newton, 'time': newton_time, 'msg': msg_newton}\n    }\n\n# Запускаем сравнение на нескольких датасетах\ndatasets = ['w8a', 'gisette', 'real-sim']\nresults = {}\n\nfor dataset in datasets:\n    try:\n        results[dataset] = run_logreg_comparison(dataset)\n    except Exception as e:\n        print(f\"Ошибка при обработке {dataset}: {e}\")\n\n# Визуализация результатов\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\nfor idx, (dataset, result) in enumerate(results.items()):\n    # График 1: Значение функции от времени\n    ax = axes[0, idx]\n    \n    if result['gd']['history']:\n        time_gd = result['gd']['history']['time']\n        func_gd = result['gd']['history']['func']\n        ax.semilogy(time_gd, func_gd, 'b-', linewidth=2, label='GD')\n    \n    if result['newton']['history']:\n        time_newton = result['newton']['history']['time']\n        func_newton = result['newton']['history']['func']\n        ax.semilogy(time_newton, func_newton, 'r-', linewidth=2, label='Newton')\n    \n    ax.set_xlabel('Время (с)')\n    ax.set_ylabel('f(x_k)')\n    ax.set_title(f'{dataset}: Значение функции\\nm={result[\"m\"]}, n={result[\"n\"]}')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    # График 2: Относительная норма градиента от времени\n    ax = axes[1, idx]\n    \n    if result['gd']['history']:\n        time_gd = result['gd']['history']['time']\n        grad_norm_gd = result['gd']['history']['grad_norm']\n        grad_norm_0 = grad_norm_gd[0] if grad_norm_gd else 1\n        rel_grad_gd = [gn**2 / grad_norm_0**2 for gn in grad_norm_gd]\n        ax.semilogy(time_gd, rel_grad_gd, 'b-', linewidth=2, label='GD')\n    \n    if result['newton']['history']:\n        time_newton = result['newton']['history']['time']\n        grad_norm_newton = result['newton']['history']['grad_norm']\n        grad_norm_0 = grad_norm_newton[0] if grad_norm_newton else 1\n        rel_grad_newton = [gn**2 / grad_norm_0**2 for gn in grad_norm_newton]\n        ax.semilogy(time_newton, rel_grad_newton, 'r-', linewidth=2, label='Newton')\n    \n    ax.set_xlabel('Время (с)')\n    ax.set_ylabel('||∇f(x_k)||² / ||∇f(x_0)||²')\n    ax.set_title(f'{dataset}: Относительная норма градиента')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('logreg_comparison.png', dpi=150)\nplt.show()\n\n# Анализ сложности методов\nprint(\"\"\"\nАнализ сложности методов:\n\nГрадиентный спуск:\n- Стоимость одной итерации: O(m * n) - вычисление градиента (A^T * (b * sigma(-b*Ax)))\n- Память: O(n) - хранение вектора x и градиента\n- Сходимость: линейная, O(κ log(1/ε)) итераций\n\nМетод Ньютона:\n- Стоимость одной итерации: O(n^3) - решение системы с матрицей Гессе\n- Память: O(n^2) - хранение матрицы Гессе\n- Сходимость: квадратичная вблизи решения, O(log log(1/ε)) итераций\n\nВыводы по эксперименту 3:\n1. Для задач малой размерности (n < 1000) метод Ньютона сходится за \n   значительно меньшее число итераций и часто быстрее по времени.\n2. Для задач большой размерности (n > 1000) градиентный спуск может быть \n   предпочтительнее из-за низкой стоимости итерации O(m*n) против O(n^3).\n3. Метод Ньютона требует хранения матрицы Гессе O(n^2), что может быть \n   проблематично для задач с n > 10^5.\n4. Выбор метода зависит от соотношения m и n, а также от требуемой точности.\n\"\"\")\n\n# ## Проверка с помощью presubmit_tests.py\n\n# Запуск тестов\n!nosetests3 presubmit_tests.py",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "invalid syntax (1210464727.py, line 579)",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 579\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install scikit-learn\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "d8b9c4dc-0bcf-49e5-8fb3-44d755d0a7be",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}